{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9bf174a-164b-4ac0-bb36-9128f4653bcf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc0343-a326-4747-8d9d-756f207c5c20",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4447598b-1752-4a47-833f-ec4378849d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import losses\n",
    "import tensorflow_text as tfText\n",
    "\n",
    "from transformers import TFBertModel, BertConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bc9ce-72bc-4ecc-b78f-44f83d5216f3",
   "metadata": {},
   "source": [
    "## Initialize notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e1aaab-0d4c-400c-a768-7a028f65e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pipeline parameters:\n",
    "training_data_folder = \"datasets/test_training data\"\n",
    "fred_series_id = \"GDPC1\"\n",
    "num_threads = 8 # For tf.data.Dataset.map operations\n",
    "batch_size = 2\n",
    "\n",
    "# General parameters\n",
    "n_vocab = 100000\n",
    "label_seq_length = 20\n",
    "series_seq_length = 40\n",
    "\n",
    "# Model hyperparameters\n",
    "dropout_rate = 0.1 # for Attention blocks\n",
    "num_heads = 2 # for Attention blocks\n",
    "decoder_stack_height = 1\n",
    "dv1 = 12 # Dimensionality of key/query for each attention head. Must be divisible by num_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14646f54-cab8-4d0b-9de2-6424dfd47a5f",
   "metadata": {},
   "source": [
    "# Create input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32d1c6-1c96-47e1-995a-ca24013b7036",
   "metadata": {},
   "source": [
    "## Load training data from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f388615e-ad51-4f47-9564-f5c31f2fbfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(path, fred_series_id, return_filepaths):\n",
    "    data_folder = os.path.basename(path)\n",
    "    pattern = r\".+_(.+)\"\n",
    "    matches = re.match(pattern, data_folder)\n",
    "    prefix = matches.group(1)\n",
    "    x1_filepath = os.path.join(path, prefix + \".txt\")\n",
    "    x2_filepath = os.path.join(path, fred_series_id, fred_series_id + \"_series.csv\")\n",
    "    y_filepath = os.path.join(path, fred_series_id, fred_series_id + \"_label.csv\")\n",
    "    paths = [x1_filepath, x2_filepath, y_filepath]\n",
    "\n",
    "    if return_filepaths:\n",
    "        return paths\n",
    "    else:\n",
    "        file_contents = []\n",
    "        for path in paths:\n",
    "            file_contents.append(tf.io.read_file(path))\n",
    "        return file_contents\n",
    "\n",
    "# Get folders that are valid training examples\n",
    "training_example_folders = []\n",
    "for folder in os.listdir(training_data_folder):\n",
    "    path = os.path.join(training_data_folder, folder)\n",
    "    if os.path.isdir(path):\n",
    "        training_example_folders.append(path)\n",
    "\n",
    "# Get feature and label filepaths from valid training example folders\n",
    "data_filepaths = {\"billtext\": [],\n",
    "                  \"series\": [],\n",
    "                  \"label\": []}\n",
    "for training_example in training_example_folders:\n",
    "    x1, x2, y = get_training_data(training_example, fred_series_id, return_filepaths=True)\n",
    "    data_filepaths[\"billtext\"].append(x1)\n",
    "    data_filepaths[\"series\"].append(x2)\n",
    "    data_filepaths[\"label\"].append(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7326f25-a1c0-462e-8255-7f3547808cd4",
   "metadata": {},
   "source": [
    "## Create Tensorflow input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7bcfb-9c2e-48e4-8310-fbfab9a5135f",
   "metadata": {},
   "source": [
    "### Create functions for dataset transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f4a733c-37ca-4d52-9c53-9e6693bc4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(test_split, dataset, **kwargs):\n",
    "    default_batch_size = 32\n",
    "    default_num_parallel_calls = 8\n",
    "\n",
    "    temp = dataset.shuffle(1000, reshuffle_each_iteration=False)\n",
    "    divisor = round(100/test_split)\n",
    "\n",
    "    def is_test(x, y):\n",
    "        return x % divisor == 0\n",
    "\n",
    "    def is_train(x, y):\n",
    "        return not is_test(x, y)\n",
    "\n",
    "    recover = lambda x,y: y\n",
    "\n",
    "    test_dataset = temp.enumerate() \\\n",
    "                        .filter(is_test) \\\n",
    "                        .map(recover)\n",
    "    train_dataset = temp.enumerate() \\\n",
    "                        .filter(is_train) \\\n",
    "                        .map(recover)\n",
    "\n",
    "    if 'batch_size' in kwargs and 'num_parallel_calls' in kwargs:\n",
    "        test_dataset = test_dataset.batch(\n",
    "            batch_size=kwargs['batch_size'],\n",
    "            num_parallel_calls=kwargs['num_parallel_calls']\n",
    "        )\n",
    "        train_dataset = train_dataset.batch(\n",
    "            batch_size=kwargs['batch_size'],\n",
    "            num_parallel_calls=kwargs['num_parallel_calls']\n",
    "        )\n",
    "    elif 'batch_size' in kwargs:\n",
    "        test_dataset = test_dataset.batch(\n",
    "            batch_size=kwargs['batch_size'],\n",
    "            num_parallel_calls=default_num_parallel_calls\n",
    "        )\n",
    "        train_dataset = train_dataset.batch(\n",
    "            batch_size=kwargs['batch_size'],\n",
    "            num_parallel_calls=default_num_parallel_calls\n",
    "        )\n",
    "    else:\n",
    "        test_dataset = test_dataset.batch(\n",
    "            batch_size=default_batch_size,\n",
    "            num_parallel_calls=default_num_parallel_calls\n",
    "        )\n",
    "        train_dataset = train_dataset.batch(\n",
    "            batch_size=default_batch_size,\n",
    "            num_parallel_calls=default_num_parallel_calls\n",
    "        )\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# This is where we pull out the actual data we want\n",
    "def load_data(path, is_csv=False):\n",
    "    if is_csv:\n",
    "        decoded_path = bytes.decode(path.numpy())\n",
    "        x = np.loadtxt(decoded_path, dtype=str, delimiter=\",\", skiprows=1)\n",
    "        relevant_series = x[:,3].astype(float).round().astype('int32')  # reduce to int for homogeneity across fred series\n",
    "        x = tf.constant(relevant_series)\n",
    "    else:\n",
    "        x = tf.io.read_file(path)\n",
    "    return x\n",
    "\n",
    "def set_shape(item, shape):\n",
    "    if not isinstance(shape, list):\n",
    "        raise ValueError(\"shape must be a List\")\n",
    "    item.set_shape(shape)\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c2e2c-4b26-403c-a231-9b684c76850f",
   "metadata": {},
   "source": [
    "### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a9b003b-cd4d-4ccd-81b5-f55ea53e3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hark! A pipeline!\n",
    "for key in data_filepaths:\n",
    "    data_filepaths[key] = tf.data.Dataset.from_tensor_slices(data_filepaths[key])\n",
    "\n",
    "# Do dataset mappings\n",
    "my_tokenizer = layers.IntegerLookup(vocabulary=np.arange(n_vocab, dtype='int32'))\n",
    "seq_length = {\"series\": series_seq_length, \"label\": label_seq_length}\n",
    "data = {}\n",
    "for key in data_filepaths:\n",
    "    if key == \"billtext\":\n",
    "        # Process billtext data\n",
    "        data[key] = data_filepaths[key] \\\n",
    "        .map(lambda x: tf.py_function(load_data, [x, False], tf.string),\n",
    "             num_parallel_calls=num_threads) \\\n",
    "        .map(lambda item: set_shape(item, []),\n",
    "             num_parallel_calls=num_threads)\n",
    "    else:\n",
    "        # Process label and series data\n",
    "        data[key] = data_filepaths[key] \\\n",
    "        .map(lambda x: tf.py_function(load_data, [x, True], tf.int32),\n",
    "             num_parallel_calls=num_threads) \\\n",
    "        .map(lambda item: set_shape(item, [seq_length[key]]),\n",
    "             num_parallel_calls=num_threads) \\\n",
    "        .map(my_tokenizer.call, num_parallel_calls=num_threads)\n",
    "\n",
    "# Zip\n",
    "training_data = tf.data.Dataset.zip((data[\"billtext\"], data[\"series\"]), data[\"label\"])\n",
    "train_data, test_data = get_train_test_split(20,\n",
    "                                             training_data,\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_parallel_calls=num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c1494-d95e-46a7-979d-8ff24a86838f",
   "metadata": {},
   "source": [
    "## Test input pipeline (run if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39fbc5-8ef1-4618-bf49-6cef87a7329e",
   "metadata": {},
   "source": [
    "### Element shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5f69d-a418-43ed-bd2b-3d6e280917ba",
   "metadata": {},
   "source": [
    "Let's see what our Dataset elements look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98283725-25af-4753-b8a2-19f34bfe5021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       "  TensorSpec(shape=(None, 40), dtype=tf.int64, name=None)),\n",
       " TensorSpec(shape=(None, 20), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "143ceea0-8b2a-4ff2-8c15-b25c3a9d6604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
       "  TensorSpec(shape=(None, 40), dtype=tf.int64, name=None)),\n",
       " TensorSpec(shape=(None, 20), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19937b41-2204-44db-9e1c-611ba45bcc1e",
   "metadata": {},
   "source": [
    "### Element values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512c43f-9631-4afb-b08c-0f73f1333629",
   "metadata": {},
   "source": [
    "Let's see the data in each element (warning: the two cells below have large outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc06356-ef76-4c5a-b5f6-dbd9927150e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_data.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516efe0-1b82-4b99-bbb4-5ee2b7783d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in test_data.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8da19-3176-453a-b541-c550ce8bd433",
   "metadata": {},
   "source": [
    "So far we've created a tf.data.Dataset object with two components. The first component is a tuple (bill_text, pre-bill_series), containing the text of the legislative bill in the 0th index and FRED series (pre-bill) data in the 1st index. These are the features. The second component contains the label, which is FRED series post-bill data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1bd7c-8b16-47d3-ad48-252fae30c6bc",
   "metadata": {},
   "source": [
    "# Create our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa161d-1d3d-479b-89ec-8d680147da64",
   "metadata": {},
   "source": [
    "## Create ancillary layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0eb802f-2f19-4c92-a765-5379073d71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        # output_dim: dimensionality of positional encoding vector\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        entry = np.tile(np.expand_dims(np.arange(inputs.shape[1]), -1), (1, self.output_dim))\n",
    "        two_i = np.tile(np.repeat(np.arange(0, self.output_dim, 2), 2), (inputs.shape[1],1))\n",
    "        if not self.output_dim % 2 == 0:\n",
    "            two_i = two_i[:,:-1]\n",
    "        base = 10000*np.ones([inputs.shape[1], self.output_dim])\n",
    "        quotient = entry/(np.power(base,(two_i/self.output_dim)))\n",
    "        sin_mask = np.tile(np.arange(self.output_dim), (inputs.shape[1],1)) % 2 == 0\n",
    "        cos_mask = np.logical_not(sin_mask)\n",
    "        output = sin_mask*np.sin(quotient) + cos_mask*np.cos(quotient)\n",
    "        return output\n",
    "\n",
    "class BaseAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, value_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads, key_dim, value_dim, dropout=dropout_rate)\n",
    "    # add a build function for mha (per docs)?\n",
    "    def build(self, inputs):\n",
    "        # super().build()\n",
    "        self.mha._build_from_signature(query=inputs, value=inputs, key=inputs)\n",
    "\n",
    "class SimpleSelfAttention(BaseAttention):\n",
    "    def call(self, inputs):\n",
    "        x = self.mha(query=inputs, value=inputs, key=inputs)\n",
    "        x = self.add([x, inputs])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCrossAttention(BaseAttention):\n",
    "    def call(self, inputs, context):\n",
    "        x = self.mha(query=inputs, value=context, key=context)\n",
    "        x = self.add([x, inputs])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCausalSelfAttention(BaseAttention):\n",
    "    def call(self, inputs):\n",
    "        x = self.mha(query=inputs, value=inputs, key=inputs, use_causal_mask=True)\n",
    "        x = self.add([x, inputs])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class FeedForwardNN(layers.Layer):\n",
    "    def __init__(self, output_dim, ff_dropout_rate):\n",
    "        super().__init__()\n",
    "        self.relu = layers.Dense(units=output_dim, activation='relu')\n",
    "        self.linear = layers.Dense(units=output_dim)\n",
    "        self.dropout = layers.Dropout(ff_dropout_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.relu(inputs)\n",
    "        x = self.linear(x)\n",
    "        x = self.add([x, inputs])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class MyCustomDecoderLayer(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sa_num_heads,\n",
    "                 ca_num_heads,\n",
    "                 sa_key_dim,\n",
    "                 ca_key_dim,\n",
    "                 sa_value_dim,\n",
    "                 ca_value_dim,\n",
    "                 ca_dropout_rate,\n",
    "                 ff_dropout_rate):\n",
    "        super().__init__()\n",
    "        self.msa = SimpleCausalSelfAttention(sa_num_heads, sa_key_dim, sa_value_dim, 0.0) # masked self attention, no dropout\n",
    "        self.ca = SimpleCrossAttention(ca_num_heads, ca_key_dim, ca_value_dim, ca_dropout_rate) # cross attention\n",
    "        self.ffn = FeedForwardNN(output_dim, ff_dropout_rate)\n",
    "\n",
    "    def call(self, inputs, context):\n",
    "        x = self.msa(inputs)\n",
    "        x = self.ca(x, context)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2148444-4641-48de-aff5-f9909330618d",
   "metadata": {},
   "source": [
    "## Create MyCustomDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd32bd0a-0095-4ef9-a0c1-5bd9c81653c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDecoder(layers.Layer):\n",
    "    def __init__(self, stack_height, d_model, h_model, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.stack_height = stack_height\n",
    "        self.decoder_layers = [MyCustomDecoderLayer(dv1,\n",
    "                                                  h_model,\n",
    "                                                  h_model,\n",
    "                                                  d_model,\n",
    "                                                  d_model,\n",
    "                                                  int(d_model/h_model),\n",
    "                                                  int(d_model/h_model),\n",
    "                                                  dropout_rate,\n",
    "                                                  dropout_rate)\n",
    "                               for _ in range(stack_height)]\n",
    "\n",
    "    def call(self, input, context):\n",
    "        x = input\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, context)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c183b-32fa-4b10-92b9-1eb677e35bb7",
   "metadata": {},
   "source": [
    "## Create MyCustomModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "004ada3f-12d5-4cb8-be7e-793c4515c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertTokenizer(layers.Layer):\n",
    "    START_TOKEN = 101\n",
    "    END_TOKEN = 102\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tfText.BertTokenizer(\"datasets/Bert_Vocabulary.txt\")\n",
    "    def call(self, inputs):\n",
    "        tokenized = self.tokenizer.tokenize(tf.strings.lower(inputs)).merge_dims(-2, -1)\n",
    "        processed_segments, segment_ids = tfText.combine_segments([tokenized],\n",
    "                                            MyBertTokenizer.START_TOKEN,\n",
    "                                            MyBertTokenizer.END_TOKEN)\n",
    "        processed_segments = tf.cast(processed_segments.to_tensor(), dtype=tf.int32)\n",
    "        segment_ids = tf.cast(segment_ids.to_tensor(), dtype=tf.int32)\n",
    "        return {'input_ids': processed_segments,\n",
    "                'token_type_ids': segment_ids}\n",
    "\n",
    "class BertEncoder(layers.Layer):\n",
    "    def __init__(self, projection_dim):\n",
    "        super().__init__()\n",
    "        # configuration = BertConfig(max_position_embeddings=max_seq_length) # awaiting HF team response on Github issue\n",
    "        self.tokenizer = MyBertTokenizer()\n",
    "        self.bert = TFBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "        self.broadcaster = layers.Dense(units=projection_dim)\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.tokenizer(input)\n",
    "        x = self.bert(**x)\n",
    "        x = tf.expand_dims(self.broadcaster(x.pooler_output), 1) # dimension issue maybe?\n",
    "        return x\n",
    "\n",
    "class MyCustomModel(keras.Model):\n",
    "    def __init__(self, decoder_stack_height, d_model, h_model, decoder_dropout_rate, n_decoder_vocab, label_seq_length, ):\n",
    "        super().__init__()\n",
    "        self.encoder = BertEncoder(d_model)\n",
    "        self.decoder = MyCustomDecoder(decoder_stack_height,\n",
    "                                       d_model,\n",
    "                                       h_model,\n",
    "                                       decoder_dropout_rate)\n",
    "        self.tokenizer = layers.IntegerLookup(vocabulary=np.arange(n_decoder_vocab, dtype='int32'))\n",
    "        self.embedding = layers.Embedding(n_decoder_vocab+1, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model)\n",
    "        self.output_layer = layers.Dense(units=n_decoder_vocab+1)\n",
    "\n",
    "    def call(self, input):\n",
    "        # Input is a tuple\n",
    "        bill_text, prebill_series = input\n",
    "        context = self.encoder(bill_text)\n",
    "\n",
    "        # Input is a list of ids (tokenized)\n",
    "        dec_inp = self.embedding(prebill_series) # creates embedding values for each item in the list. Output shae: [Ty, dv1]\n",
    "        for i in tqdm(range(label_seq_length)): # not efficient cuz context vectors recomputed every time\n",
    "            x = dec_inp + self.positional_encoding(dec_inp) # augments embedding id's with positional data. Output shape: [Ty, dv1]\n",
    "            x = self.decoder(x, context)\n",
    "            # Get last item\n",
    "            new_token_emb = tf.expand_dims(x[:,-1,:], 1)\n",
    "            dec_inp = tf.concat([dec_inp, new_token_emb],1)\n",
    "\n",
    "        x = dec_inp[:,-label_seq_length:,:]\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "074391a9-e54c-4c48-ada7-48946373bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "my_model = MyCustomModel(decoder_stack_height,\n",
    "                         dv1,\n",
    "                         num_heads,\n",
    "                         dropout_rate,\n",
    "                         n_vocab,\n",
    "                         label_seq_length)\n",
    "my_model.compile(optimizer=Adam(),\n",
    "                 loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c3708-1335-4db4-9190-6bf659d16bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(train_data, epochs=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc72e690-69d4-4e96-b401-7cc9258dc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = my_model.evaluate(test_data)\n",
    "print(my_model.metrics_names)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
