{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4df1a0-f78c-40b5-b235-a0f82cff948c",
   "metadata": {},
   "source": [
    "# WeThePeople - Predicting how US legislature affects the US Economy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f17292-ee0d-46f7-bb34-40eb9e86d832",
   "metadata": {},
   "source": [
    "The American economic machine has long been a fascination of mine. I've been keenly interested in understanding with great depth, the chain of cause-effect relationships that make the american economy tick. Perhaps more importantly, understanding this provides a framework and springboard to understand the rest of the global economy at large.\n",
    "\n",
    "The American economy is a multi-dimensional system; it ingests mutliple types of signals from multiple sources and similarly can be measured along multiple axes. In this project, we pare down this immense problem into something rather bite-sized. It's no secret that the actions of those in government affect the lives of the constituency. The question this project sets out to answer is: How? Can we understand exactly how different pieces of legislature affects the American economy as a whole? And armed with this knowledge, can we predict how proposed legislature might affect the American economy in the future?\n",
    "\n",
    "We'll use machine learning models to aid us in answering these questions. Without further ado...let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3bc0ab-178e-4d8a-bda1-ec875d7651fe",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53174861-a53e-49e1-87b1-272d34277862",
   "metadata": {},
   "source": [
    "I considered getting data for this project from the following sources:\n",
    "\n",
    "| Source | Description |\n",
    "|---|---|\n",
    "| Congress.GOV | Library of the US Congress: Collects records on many (if not all) congressional activities. Perhaps most important for us, it contains an archive of all Public Laws in US History. |\n",
    "| FRED | Federal Reserve Economic Data: Aggregates multiple economic indicators from multiple government organizations and publishes them in a single repository. This is probably the best one-stop shop for anything US Economic Data related. |\n",
    "| BEA | Bureau of Economic Analysis: BEA's economists produce some of the world's most closely watched statistics, including U.S. gross domestic product, better known as GDP. We do state and local numbers, too, plus foreign trade and investment stats and industry data |\n",
    "\n",
    "In fact I built Python APIs from scratch for both Congress.GOV and BEA. However upon further investigation, turns out the data from the BEA is readily available via FRED. So we'll press forward with just Congress.GOV and FRED as our data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc763828-a13c-4205-995c-4cd4c5d9d919",
   "metadata": {},
   "source": [
    "### Sourcing and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df0e6b4-fe8c-424a-85bf-9a28bfd53209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import all_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66310c0-5e89-490e-a6fd-24ad320a2c33",
   "metadata": {},
   "source": [
    "**BEWARE:** The cell below downloads *a lot* of data from the FRED API and Congress.GOV API. Due to server side rate limits, this cell could run for hours, perhaps even days. If you choose to, you can mitigate this by assigning the `search_limit` key in the `congress_args` dictionary, and/or assigning the `min_popularity` key in the `fred_args` dictionary. See documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906087e-ecb0-4886-b1a2-9d5962f17b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'compile_congress_dataset': False,\n",
    "    'compile_fred_dataset': True,\n",
    "    'retry_congress_errors': False,\n",
    "    'congress_args': {},\n",
    "    'fred_args': {},\n",
    "    'retry_args': {}\n",
    "}\n",
    "\n",
    "all_datasets.get(**kwargs) # THIS SHOULD RETURN SERIES_SEQ_LENGTH AND LABEL_SEQ_LENGTH\n",
    "# ALSO WE NEED TO PUT Bert_Vocabulary in the right place to be read..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e712e-8756-4288-8d9d-94da273c949f",
   "metadata": {},
   "source": [
    "### Tensorflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be668e7d-cdb8-4ec3-b145-365f275214fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling.preprocess import tf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25211d-503d-4262-8b96-2e2639b6f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come back to this\n",
    "pipeline_kwargs = {\n",
    "    'training_data_folder': \"datasets/training data\",\n",
    "    'fred_series_id': \"GDPC1\",\n",
    "    'series_seq_length': None,\n",
    "    'label_seq_length': None,\n",
    "    'n_vocab': None,\n",
    "    'num_threads': None,\n",
    "    'local_batch_size': None,\n",
    "    'distributed': None\n",
    "}\n",
    "\n",
    "train_data, val_data, test_data, steps_per_epoch = tf_pipeline.build(**pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9f6a1-8a4e-48af-971c-be006cc8a133",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3235485-0890-4d14-99eb-5a46f5980686",
   "metadata": {},
   "source": [
    "Blah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1a42a-0f39-4746-bed2-9bc8e8214269",
   "metadata": {},
   "source": [
    "### Setup (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa51769-1138-476e-b180-0fb32cb7c6f8",
   "metadata": {},
   "source": [
    "For reasons that will become apparent later, we'll need to run the machine learning aspect of this project on AWS (Amazon Web Services). Specifically we'll be using Amazon Sagemaker for training our ML model and Amazon S3 for storing our training data and model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01aa695-65fd-4d9e-88fd-4eccdd34405f",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1893fd-ef74-46da-8450-34989fa5ab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/tomi/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/tomi/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    "    WarmStartConfig,\n",
    "    WarmStartTypes\n",
    ")\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "from src.aws_utils import upload\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56571dd8-c4fb-4376-8663-de81b977e8b0",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fde72-3748-416c-b33d-cebf4b79da99",
   "metadata": {},
   "source": [
    "Let's define some convenience methods that we'll use later on in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a56f8fdd-c956-40c8-bdf6-146bac936793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_estimator(hparams_dict, n_instances=1):\n",
    "    estimator = TensorFlow(\n",
    "        entry_point=\"ML.py\",\n",
    "        role=role,\n",
    "        source_dir=source_uri,\n",
    "        model_dir=model_uri,\n",
    "        framework_version=\"2.13\",\n",
    "        py_version=\"py310\",\n",
    "        instance_type=\"ml.g5.xlarge\",\n",
    "        instance_count=n_instances,\n",
    "        volume_size=20,\n",
    "        output_path=output_uri,\n",
    "        hyperparameters=hparams_dict\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "\n",
    "def get_distributed_estimator(hparams_dict, n_instances=1):\n",
    "    if hparams_dict[\"distributed\"] == \"tf\":\n",
    "        dist_config = {\"multi_worker_mirrored_strategy\": {\"enabled\": True}}\n",
    "        env_vars = None\n",
    "    elif hparams_dict[\"distributed\"] == \"horovod\":\n",
    "        dist_config = {\"mpi\": {\"enabled\": True}}\n",
    "        env_vars = {\"HOROVOD_GPU_OPERATIONS\": \"NCCL\"}\n",
    "\n",
    "    estimator = TensorFlow(\n",
    "        entry_point=\"ML.py\",\n",
    "        role=role,\n",
    "        source_dir=source_uri,\n",
    "        model_dir=model_uri,\n",
    "        framework_version=\"2.13\",\n",
    "        py_version=\"py310\",\n",
    "        instance_type=\"ml.g5.12xlarge\",\n",
    "        instance_count=n_instances,\n",
    "        volume_size=20,\n",
    "        output_path=output_uri,\n",
    "        hyperparameters=hparams_dict,\n",
    "        distribution=dist_config,\n",
    "        environment=env_vars,\n",
    "        checkpoint_s3_uri=checkpoint_uri\n",
    "    )\n",
    "    return estimator\n",
    "\n",
    "# TODO: Add another function with file_mode as \"pipe\" or \"fast_file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a01a94-074f-4a3a-bd73-f6cec340bdbc",
   "metadata": {},
   "source": [
    "#### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143095a4-c82b-4fa8-8176-531dbb93a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = \"my_first_model\"\n",
    "source_endpt = f\"{model_prefix}/inputs/source\"\n",
    "inputs_endpt = f\"{model_prefix}/inputs/datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb148e8-e3f9-4834-a53b-fe60bfb06842",
   "metadata": {},
   "source": [
    "##### Upload source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611e88c-bddf-4a57-8e6e-fd695e6721a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = tarfile.open(\"WeThePeople.tgz\", 'w:gz')\n",
    "for item in os.listdir(\"src\"):\n",
    "    tar.add(os.path.join(\"src\", item), item)\n",
    "\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667dfbb-ef1d-4595-8f89-410bce3eedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"WeThePeople.tgz\"\n",
    "inputs = sagemaker_session.upload_data(path=local_dir, bucket=bucket, key_prefix=source_endpt)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f09a75-5590-4d67-85eb-10b302e8dbd3",
   "metadata": {},
   "source": [
    "##### Upload training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52bea82-944d-40f5-9a80-9986381c0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script parameters\n",
    "local_dir = 'datasets/training data'\n",
    "key_prefix = \"my_first_model/inputs/\"\n",
    "n_threads = 20\n",
    "bucket = os.environ['MY_DEFAULT_S3_BUCKET']\n",
    "\n",
    "upload(local_dir, bucket, key_prefix, n_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49923d4-dbda-49a6-932e-7165022dd37f",
   "metadata": {},
   "source": [
    "#### Estimator arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ebee20-296e-497b-a9d8-fcec28e41ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_uri = f\"s3://{bucket}/{inputs_endpt}/training data\"\n",
    "model_uri = f\"s3://{bucket}/{model_prefix}/outputs/model\"\n",
    "output_uri = f\"s3://{bucket}/{model_prefix}/outputs/output\"\n",
    "source_uri = f\"s3://{bucket}/{source_endpt}/WeThePeople.tgz\"\n",
    "checkpoint_uri = f\"s3://{bucket}/{model_prefix}/outputs/checkpoints\"\n",
    "channels = {\n",
    "    \"train\": train_data_uri\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4132880-0aff-4098-9198-cc3ada31a382",
   "metadata": {},
   "source": [
    "### Model v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503783c-423d-4bcc-b2ba-65bd026fbbfa",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba4da9-4a47-4f34-9fac-ae93c703678d",
   "metadata": {},
   "source": [
    "This model is the standard encoder-decoder transformer architecture from the `Attention is All You Need` paper. The encoder is Google's DistilBERT, and the decoder is one I built myself, capable of properly processing time-series and performing regression to predict the next token.\n",
    "\n",
    "In a nutshell, the encoder ingests the legislative text and exposes the corresponding encoded values to the decoder. The decoder ingests econometric time-series data starting from a certain date, leading up to the date the bill in question was signed into law, then employs cross-attention to ingest the encoded values from the encoder, and finally uses a regression head to output its next prediction in the time-series. Note that the cross attention layer can be repeated N times, just as in the original paper. This entire process is repeated auto-regressively until we've generated a time-series long enough to suit our needs.\n",
    "\n",
    "Below, I train this model to predict a 5-year econometric outlook given a legislative bill and 10 years worth of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479fc5a-d4bf-41be-b59e-268618f8886b",
   "metadata": {},
   "source": [
    "#### Training on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df09db-beda-4b5b-b467-c429008cb94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize as needed\n",
    "hparams = {\n",
    "    \"num_threads\": 2,\n",
    "    \"batch_size_per_worker\": 1,\n",
    "    \"n_vocab\": 100000,\n",
    "    \"label_seq_length\": 20,\n",
    "    \"series_seq_length\": 40,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"num_heads\": 2,\n",
    "    \"stack_height\": 1,\n",
    "    \"d_values\": 12,\n",
    "    \"d_keys\": 12,\n",
    "    \"encoder_max_seq_len\": 512,\n",
    "    \"epochs\": 3,\n",
    "    \"learning_rate\": 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764a9af-189a-4ea4-8016-c96fad35db15",
   "metadata": {},
   "source": [
    "##### Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c123809-f7a0-4d2f-8e2d-7e00624cf0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams[\"distributed\"] = \"horovod\"\n",
    "estimator = get_distributed_estimator(hparams)\n",
    "estimator.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9233e0-cc81-4fb0-9363-d632c854cbef",
   "metadata": {},
   "source": [
    "##### Non-distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7353ef-a59c-4a18-a9a8-dd1fbf6a2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = get_small_estimator(hparams)\n",
    "estimator.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbf5b9-527f-47cd-8e82-aee35cd56eb2",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e55506a-7227-4bd3-b13a-942a4e356bd0",
   "metadata": {},
   "source": [
    "I ran a hyperparameter tuning job to arrive at the best performing model of which the results are reported below. The hyperparameters for the best performing model are as follows:\n",
    "\n",
    "|Hyperparameter Name|Description|Value|\n",
    "|---|---|---|\n",
    "|batch_size_per_worker|Number of samples per gradient update; also number of samples processed in parallel|3\n",
    "|dropout_rate|Dropout probability for dropout layers in decoder|0.0167\n",
    "|num_heads|Number of heads in each Multi-head attention block in decoder|12\n",
    "|stack_height|Number of \"decoder layers\" in the decoder. Each decoder layer consists of a Multi Head Attention, Add & Norm, and Feed Forward NN block|5\n",
    "|d_values|Dimensionality of transformer values in decoder|481\n",
    "|d_keys|Dimensionality of transformer keys in decoder|245\n",
    "|encoder_max_seq_len|Maximum sequence length that the encoder accepts as input|888\n",
    "|learning_rate|Learning rate for the model compiler|0.0058\n",
    "\n",
    "***\n",
    "\n",
    "This model achieved a 10.48% MAPE (Mean Absolute Percentage Error) on the GDCP1 (Real Gross Domestic Product) timeseries with a 10 year look-back and 5 year look-ahead window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e320cc1-fe32-485a-8fc2-11b671e04e22",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d061ad5-c1b1-4cc3-84df-2fee30b2a9e0",
   "metadata": {},
   "source": [
    "If the user chooses, run the cells below to perform hyperparameter tuning on any of the models above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3988de0-eaa8-45a9-bfe8-bac3ca3e7d66",
   "metadata": {},
   "source": [
    "#### Setup Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c6779-5e58-4eee-81b7-65f16bc61f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize as needed\n",
    "static_hparams = {\n",
    "    \"epochs\": 4,\n",
    "    \"n_vocab\": 100000,\n",
    "    \"label_seq_length\": 20,\n",
    "    \"series_seq_length\": 40\n",
    "}\n",
    "\n",
    "hparam_ranges = {\"encoder_max_seq_len\": IntegerParameter(627, 900),\n",
    "                 \"batch_size_per_worker\": IntegerParameter(3, 7),\n",
    "                 \"num_threads\": IntegerParameter(17, 27),\n",
    "                 \"d_values\": IntegerParameter(447, 675),\n",
    "                 \"d_keys\": IntegerParameter(191, 300),\n",
    "                 \"num_heads\": IntegerParameter(11, 18),\n",
    "                 \"stack_height\": IntegerParameter(3, 6),\n",
    "                 \"dropout_rate\": ContinuousParameter(0, 0.2),\n",
    "                 \"learning_rate\": ContinuousParameter(1e-3, 6e-3)\n",
    "                }\n",
    "\n",
    "obj_metric_name = \"MAPE\"\n",
    "obj_type = \"Minimize\"\n",
    "metric_defs = [\n",
    "    {\n",
    "        \"Name\": \"MSE\",\n",
    "        \"Regex\": \"val_mse: ([0-9\\\\.]+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"MAPE\",\n",
    "        \"Regex\": \"val_mape: ([0-9\\\\.]+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"MAE\",\n",
    "        \"Regex\": \"val_mae: ([0-9\\\\.]+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"TEST_MSE\",\n",
    "        \"Regex\": \"test_mse: ([0-9\\\\.]+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"TEST_MAPE\",\n",
    "        \"Regex\": \"test_mape: ([0-9\\\\.]+)\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"TEST_MAE\",\n",
    "        \"Regex\": \"test_mae: ([0-9\\\\.]+)\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa890b-4c09-49ae-8beb-e0866db299b9",
   "metadata": {},
   "source": [
    "If you have a warm start config, declare it in the cell below and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88784061-64b3-4bc3-bb9c-4ab41c9af15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup warm start config\n",
    "parent_names = {\"TwelveXlargeTune-240404-2029\"}\n",
    "warm_start_config = WarmStartConfig(\n",
    "    WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM, parents=parent_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e19ae0-4b1d-45af-af0f-2e51f11210f3",
   "metadata": {},
   "source": [
    "##### Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72678d-c0a5-415e-b72e-751fcbfd253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_hparams[\"distributed\"] = \"horovod\"\n",
    "estimator = get_distributed_estimator(static_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b8c7e-ef31-40d1-a79b-4548732dbffe",
   "metadata": {},
   "source": [
    "##### Non-distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e35c16-5c04-4db8-a576-acd4fbd5221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = get_small_estimator(static_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f17ac-41fc-478f-ac74-4e780452b4b8",
   "metadata": {},
   "source": [
    "#### Initialize and Run Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b476f-9a98-4835-86b2-e1aa3e873972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    base_tuning_job_name=\"TwelveXlargeTune\",\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=obj_metric_name,\n",
    "    hyperparameter_ranges=hparam_ranges,\n",
    "    metric_definitions=metric_defs,\n",
    "    objective_type=obj_type,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=1,\n",
    "    warm_start_config=warm_start_config\n",
    ")\n",
    "\n",
    "tuner.fit(inputs=channels, include_cls_metadata=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
